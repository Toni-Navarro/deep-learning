{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPcAgkIyC6HDtiKBchIkyb5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Toni-Navarro/deep-learning/blob/master/Improving%20my%20first%20model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0D2WC2CJkPc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZH3FMjKJqAE",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "#Improving my first deep learning model\n",
        "\n",
        "We will use the preload data set from keras with several images of different handwritten digits in order to build a neural net model to identify them. We will modify the parameter to reach better accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8LI35ujJrz-",
        "colab_type": "code",
        "outputId": "4a314722-7a37-406b-b665-991e52d19a86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#Preparation of the model environment\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtItbzl1J5zg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Loading the data\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Mm3tNL5dwd4",
        "colab_type": "code",
        "outputId": "95543af8-dd2b-481d-8d17-12c477fe5bcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "(60000,)\n",
            "(10000, 28, 28)\n",
            "(10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JqFRS6K07jJs",
        "colab": {}
      },
      "source": [
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Add a channels dimension\n",
        "x_train = x_train[..., tf.newaxis]\n",
        "x_test = x_test[..., tf.newaxis]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5hdPrX90C3q",
        "colab_type": "text"
      },
      "source": [
        "**Why should the data be shuffled for machine learning tasks**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqK92HAEfCoe",
        "colab_type": "text"
      },
      "source": [
        "The process of training a neural network is to find the minimum value of a loss function LX(W), where W represents a matrix (or several matrices) of weights between neurons and X represents the training dataset. I use a subscript for X to indicate that our minimization of L occurs only over the weights W (that is, we are looking for W such that L is minimized) while X\n",
        "\n",
        "is fixed.\n",
        "\n",
        "Now, if we assume that we have P\n",
        "elements in W (that is, there are P weights in the network), L is a surface in a P+1-dimensional space. To give a visual analogue, imagine that we have only two neuron weights (P=2). Then L has an easy geometric interpretation: it is a surface in a 3-dimensional space. This arises from the fact that for any given matrices of weights W, the loss function can be evaluated on X\n",
        "\n",
        "and that value becomes the elevation of the surface.\n",
        "\n",
        "But there is the problem of non-convexity; the surface I described will have numerous local minima, and therefore gradient descent algorithms are susceptible to becoming \"stuck\" in those minima while a deeper/lower/better solution may lie nearby. This is likely to occur if X\n",
        "is unchanged over all training iterations, because the surface is fixed for a given X\n",
        "\n",
        "; all its features are static, including its various minima.\n",
        "\n",
        "A solution to this is mini-batch training combined with shuffling. By shuffling the rows and training on only a subset of them during a given iteration, X\n",
        "changes with every iteration, and it is actually quite possible that no two iterations over the entire sequence of training iterations and epochs will be performed on the exact same X. The effect is that the solver can easily \"bounce\" out of a local minimum. Imagine that the solver is stuck in a local minimum at iteration i with training mini-batch Xi. This local minimum corresponds to L evaluated at a particular value of weights; we'll call it LXi(Wi). On the next iteration the shape of our loss surface actually changes because we are using Xi+1, that is, LXi+1(Wi) may take on a very different value from LXi(Wi) and it is quite possible that it does not correspond to a local minimum! We can now compute a gradient update and continue with training. To be clear: the shape of LXi+1 will -- in general -- be different from that of LXi. Note that here I am referring to the loss function L evaluated on a training set X; it is a complete surface defined over all possible values of W, rather than the evaluation of that loss (which is just a scalar) for a specific value of W\n",
        "\n",
        ". Note also that if mini-batches are used without shuffling there is still a degree of \"diversification\" of loss surfaces, but there will be a finite (and relatively small) number of unique error surfaces seen by the solver (specifically, it will see the same exact set of mini-batches -- and therefore loss surfaces -- during each epoch).\n",
        "\n",
        "One thing I deliberately avoided was a discussion of mini-batch sizes, because there are a million opinions on this and it has significant practical implications (greater parallelization can be achieved with larger batches). However, I believe the following is worth mentioning. Because L\n",
        "is evaluated by computing a value for each row of X (and summing or taking the average; i.e., a commutative operator) for a given set of weight matrices W, the arrangement of the rows of X has no effect when using full-batch gradient descent (that is, when each batch is the full X, and iterations and epochs are the same thing)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKh3UrCe0VwS",
        "colab_type": "text"
      },
      "source": [
        "batch and shuffle the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfTsl_mRdwoA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    (x_train, y_train)).shuffle(10000).batch(32)\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMy4HgMFdwvw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "877c6323-f987-4e51-f668-a8116746d161"
      },
      "source": [
        "print(train_ds)\n",
        "print(test_ds)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<BatchDataset shapes: ((None, 28, 28, 1), (None,)), types: (tf.float64, tf.uint8)>\n",
            "<BatchDataset shapes: ((None, 28, 28, 1), (None,)), types: (tf.float64, tf.uint8)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7udJgYyAQAh",
        "colab_type": "text"
      },
      "source": [
        "Building model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPX3P-KGdwys",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyModel(Model):\n",
        "  def __init__(self):\n",
        "    super(MyModel, self).__init__()\n",
        "    self.conv1 = Conv2D(32, 3, activation='relu')\n",
        "    self.flatten = Flatten()\n",
        "    self.d1 = Dense(128, activation='relu')\n",
        "    self.d2 = Dense(10)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.flatten(x)\n",
        "    x = self.d1(x)\n",
        "    return self.d2(x)\n",
        "\n",
        "# Create an instance of the model\n",
        "model = MyModel()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZSjvRj9Cfip",
        "colab_type": "text"
      },
      "source": [
        "Choose an optimizer and loss function for training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u48C9WQ774n4",
        "colab": {}
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXuElUO9CqAU",
        "colab_type": "text"
      },
      "source": [
        "Select metrics to measure the loss and the accuracy of the model. These metrics accumulate the values over epochs and then print the overall result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H26_ZZLUdw6m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9m6_iKvwDhyL",
        "colab_type": "text"
      },
      "source": [
        "Use tf.GradientTape to train the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ARUBZ4Ndw9S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(images, labels):\n",
        "  with tf.GradientTape() as tape:\n",
        "    # training=True is only needed if there are layers with different\n",
        "    # behavior during training versus inference (e.g. Dropout).\n",
        "    predictions = model(images, training=True)\n",
        "    loss = loss_object(labels, predictions)\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "  train_loss(loss)\n",
        "  train_accuracy(labels, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0ZT9l8aDpPH",
        "colab_type": "text"
      },
      "source": [
        "Test the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqsmy1H5dxDX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def test_step(images, labels):\n",
        "  # training=False is only needed if there are layers with different\n",
        "  # behavior during training versus inference (e.g. Dropout).\n",
        "  predictions = model(images, training=False)\n",
        "  t_loss = loss_object(labels, predictions)\n",
        "\n",
        "  test_loss(t_loss)\n",
        "  test_accuracy(labels, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSy2x_7IdxGF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "36307d6b-48c9-4921-f3d8-6fbec4ef6962"
      },
      "source": [
        "EPOCHS = 5\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  # Reset the metrics at the start of the next epoch\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  test_loss.reset_states()\n",
        "  test_accuracy.reset_states()\n",
        "\n",
        "  for images, labels in train_ds:\n",
        "    train_step(images, labels)\n",
        "\n",
        "  for test_images, test_labels in test_ds:\n",
        "    test_step(test_images, test_labels)\n",
        "\n",
        "  template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
        "  print(template.format(epoch+1,\n",
        "                        train_loss.result(),\n",
        "                        train_accuracy.result()*100,\n",
        "                        test_loss.result(),\n",
        "                        test_accuracy.result()*100))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer my_model_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "Epoch 1, Loss: 0.13616277277469635, Accuracy: 95.9183349609375, Test Loss: 0.06895151734352112, Test Accuracy: 97.82999420166016\n",
            "Epoch 2, Loss: 0.04049573093652725, Accuracy: 98.72000122070312, Test Loss: 0.04830228164792061, Test Accuracy: 98.38999938964844\n",
            "Epoch 3, Loss: 0.019145004451274872, Accuracy: 99.39666748046875, Test Loss: 0.050554048269987106, Test Accuracy: 98.37999725341797\n",
            "Epoch 4, Loss: 0.012602318078279495, Accuracy: 99.55833435058594, Test Loss: 0.055954527109861374, Test Accuracy: 98.27999877929688\n",
            "Epoch 5, Loss: 0.007870126515626907, Accuracy: 99.7316665649414, Test Loss: 0.073118194937706, Test Accuracy: 98.27999877929688\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dalaPxrOdxI6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bx3fcJi-KImX",
        "colab_type": "text"
      },
      "source": [
        "Preparing the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyyN73q1KMe6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#First, we are going to transform data type from integer to float in order to normalize after that to get all values between 0 and 1\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "x_train /= 255\n",
        "x_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5XuFWEYKNie",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Now we are going to change the different images to concat all their lines to transform them to a single line\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edgt_JiXKPQn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Let's use keras method \"to_categorical\"\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFhy54YbK4oP",
        "colab_type": "text"
      },
      "source": [
        "Defining the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CptjSiKuK5V6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Our model will be sequential with a conv2d lay and relu activation, another lay which will set in one dimension the result of first lay (no parameters) and finally two dense lays\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import Flatten\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, 3, activation='relu', input_shape=(30, 30, 1)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqifcMBLVAQh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sRhXRXtQVc-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}